{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contact Center Insights generation involves two steps:\n",
    "\n",
    "1. **Transcription with speaker Diarization**\n",
    "   - **NVIDIA Riva Integration:** Transcribes incoming audio calls between two speakers using NVIDIA Riva's Parakeet CTC 1.1b ASR model and creates a structured transcript.\n",
    "\n",
    "2. **Insight Generation**\n",
    "   - **Entity Extraction:** Extracts key entities like customer and agent names, topic and subtopic of the conversation.\n",
    "    - **Agent Performance Evaluation:** Evaluates agent performance based several key metrics.\n",
    "    - **Combine Insights:** Combines all extracted insights into a structured JSON."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content Overview\n",
    "1. [Install dependencies](#Install-dependencies)\n",
    "2. [Set required environment variables](#Set-required-environment-variables)\n",
    "3. [Transcribe Audio](#Transcribe-Audio)\n",
    "4. [Generate Insights](#Generate-Insights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Set required environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from io import BytesIO\n",
    "from pydub import AudioSegment\n",
    "import riva.client\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# validate we have the required variables\n",
    "REQUIRED_VARIABLES = [\n",
    "    \"NVIDIA_PARAKEET_NIM_API_KEY\",\n",
    "    \"NVIDIA_LLAMA_NIM_API_KEY\",\n",
    "]\n",
    "\n",
    "for var in REQUIRED_VARIABLES:\n",
    "    if var not in os.environ:\n",
    "        os.environ[var] = getpass.getpass(f\"Please set the {var} environment variable.\")\n",
    "\n",
    "# optional variables\n",
    "os.environ[\"RIVA_SPEECH_API_SERVER\"] = os.getenv(\"RIVA_SPEECH_API_SERVER\", \"grpc.nvcf.nvidia.com\")\n",
    "\n",
    "# Look for audio files in the current directory with .wav format\n",
    "audio_files = [f for f in os.listdir(\"audio\") if f.endswith(\".wav\")]\n",
    "if not audio_files:\n",
    "    raise Exception(\"No .wav files found in the current directory.\")\n",
    "\n",
    "AUDIO_FILE = audio_files[0]\n",
    "print(f\"Using audio file: {AUDIO_FILE}\")\n",
    "\n",
    "# validate the audio file, it must have two channels\n",
    "audio = AudioSegment.from_file(f\"audio/{AUDIO_FILE}\")\n",
    "if audio.channels != 2:\n",
    "    raise Exception(\"Audio file must have exactly two channels.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Transcribe Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Utterance(BaseModel):\n",
    "    time: int = Field(..., description=\"Time in milliseconds when the utterance starts\")\n",
    "    speaker: int\n",
    "    spoken_words: str = Field(..., description=\"Words spoken by the speaker\")\n",
    "\n",
    "class Transcript(BaseModel):\n",
    "    utterances: List[Utterance]\n",
    "    call_duration: int = Field(..., description=\"Duration of the conversation in milliseconds\")\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"\\n\".join([f\"{u.time} - {u.speaker}: {u.spoken_words}\" for u in self.utterances])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_audio_channels(filename: str) -> tuple[BytesIO, BytesIO, int]:\n",
    "    \"\"\"Split the audio file into two channels.\"\"\"\n",
    "    audio = AudioSegment.from_file(f\"audio/{filename}\", format=\"wav\")\n",
    "\n",
    "    left_channel = audio.split_to_mono()[0]\n",
    "    right_channel = audio.split_to_mono()[1]\n",
    "\n",
    "    left_channel_bytes, right_channel_bytes = BytesIO(), BytesIO()\n",
    "    left_channel.export(left_channel_bytes, format=\"wav\")\n",
    "    right_channel.export(right_channel_bytes, format=\"wav\")\n",
    "\n",
    "    # duration of the audio in milliseconds\n",
    "    duration = len(audio)\n",
    "\n",
    "    return left_channel_bytes, right_channel_bytes, duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_with_riva(audio_bytes):\n",
    "    \"\"\"Transcribe the audio file using Riva Speech API.\"\"\"\n",
    "\n",
    "    # Authenticate with Riva Speech API\n",
    "    auth = riva.client.Auth(\n",
    "        uri=os.environ[\"RIVA_SPEECH_API_SERVER\"],\n",
    "        use_ssl=True,\n",
    "        metadata_args=[\n",
    "            ['authorization', 'Bearer {}'.format(os.environ[\"NVIDIA_PARAKEET_NIM_API_KEY\"])],\n",
    "            ['function-id', '1598d209-5e27-4d3c-8079-4751568b1081']\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Configure the transcription\n",
    "    config = riva.client.RecognitionConfig(\n",
    "        language_code=\"en-US\",\n",
    "        enable_word_time_offsets=True,      # Enables word timestamps\n",
    "        max_alternatives=1,                 # Set to 1 for single-best result\n",
    "        enable_automatic_punctuation=True,\n",
    "        audio_channel_count = 1,\n",
    "    )\n",
    "\n",
    "    riva_asr = riva.client.ASRService(auth)\n",
    "    response = riva_asr.offline_recognize(audio_bytes, config)\n",
    "    \n",
    "    return response.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_and_format_results(left_results, right_results) -> List[Utterance]:\n",
    "    \"\"\"Combine the results from the two channels and format them.\"\"\"\n",
    "    \n",
    "    def extract_transcript(results, speaker_label):\n",
    "        \"\"\"Extract the transcript and start time from the first word of each alternative.\"\"\"\n",
    "\n",
    "        transcript_results = []\n",
    "        for result in results:\n",
    "            for alternative in result.alternatives:\n",
    "                transcript_results.append({\n",
    "                    'transcript': alternative.transcript,\n",
    "                    'start_time': alternative.words[0].start_time, # Start time of the first word\n",
    "                    'speaker': speaker_label\n",
    "                })\n",
    "\n",
    "        return transcript_results\n",
    "\n",
    "    left_results = extract_transcript(left_results, 0)\n",
    "    right_results = extract_transcript(right_results, 1)\n",
    "\n",
    "    combined_results = left_results + right_results\n",
    "    # sort all utterances by start_time\n",
    "    combined_results.sort(key=lambda x: x['start_time'])\n",
    "\n",
    "    # format the results\n",
    "    utterances = []\n",
    "    for result in combined_results:\n",
    "\n",
    "        utt = Utterance(time=float(result['start_time']), speaker=result['speaker'], spoken_words=result['transcript'].strip())\n",
    "        utterances.append(utt)\n",
    "\n",
    "    return utterances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the audio into two channels and transcribe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the audio file into two channels\n",
    "left_channel, right_channel, duration = split_audio_channels(AUDIO_FILE)\n",
    "\n",
    "# transcribe the both channels individually\n",
    "left_results = transcribe_with_riva(left_channel.getvalue())\n",
    "right_results = transcribe_with_riva(right_channel.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Format the results combined form both channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine and format the results\n",
    "utterances = combine_and_format_results(left_results, right_results)\n",
    "transcript = Transcript(utterances=utterances, call_duration=duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Generate Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Entity Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "class Entities(BaseModel):\n",
    "    agent_name: Optional[str]\n",
    "    customer_name: Optional[str]\n",
    "    agent_speaker: int \n",
    "    customer_speaker: int \n",
    "    reason: Optional[str] \n",
    "    topic: str \n",
    "    subtopic: str\n",
    "\n",
    "entity_extraction_instructions = \"\"\"You are an expert analyst specialized in extracting insights from call center transcripts.\n",
    "You will extract the following information from the call transcript:\n",
    "- Agent name, if not found, use \"Unknown Agent\"\n",
    "- Customer name, if not found, use \"Unknown Customer\"\n",
    "- Speaker id for the agent, 0 or 1\n",
    "- Speaker id for the customer, 0 or 1\n",
    "- Primary reason for the call\n",
    "- Main topic of the conversation\n",
    "- More specific subtopic under the main topic\n",
    "\n",
    "Call Transcript:\n",
    "###\n",
    "{transcript}\n",
    "###\n",
    "\"\"\"\n",
    "\n",
    "llm = ChatNVIDIA(model=\"meta/llama-3.3-70b-instruct\", api_key=os.environ[\"NVIDIA_LLAMA_NIM_API_KEY\"])\n",
    "prompt = ChatPromptTemplate.from_template(entity_extraction_instructions)\n",
    "llm = llm.with_structured_output(Entities)\n",
    "chain = prompt | llm \n",
    "transcript_text = \"\\n\".join([f\"{u.time} - {u.speaker}: {u.spoken_words}\" for u in transcript.utterances])\n",
    "\n",
    "entities: Entities = chain.invoke({\n",
    "   \"transcript\": transcript_text\n",
    "})\n",
    "print(entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Agent Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScoreMetric(BaseModel):\n",
    "    value: int = Field(description=\"Score metric value from 1 to 10, 1 being the lowest and 10 being the highest\")\n",
    "    justification: str = Field(description=\"Explanation for the score metric value\")\n",
    "\n",
    "class BoolMetric(BaseModel):\n",
    "    value: int = Field(description=\"Boolean metric value, 0 for false, 1 for true\")\n",
    "    justification: str = Field(description=\"Explanation for the choice of the boolean metric\")\n",
    "\n",
    "class StringMetric(BaseModel):\n",
    "    value: str\n",
    "    justification: str = Field(description=\"Explanation of the string metric value\")\n",
    "\n",
    "\n",
    "class Evaluation(BaseModel):\n",
    "    greeting: BoolMetric\n",
    "    hold: BoolMetric\n",
    "    ticket: BoolMetric\n",
    "    listening: BoolMetric\n",
    "    understanding: BoolMetric\n",
    "    tone: BoolMetric\n",
    "    proactivity: BoolMetric\n",
    "    clarity: BoolMetric\n",
    "    resolved: BoolMetric\n",
    "    customer_sentiment: ScoreMetric\n",
    "    escalation: BoolMetric\n",
    "    agent_feedback: StringMetric\n",
    "    escalation_reason: StringMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Evaluate call transcript against metrics shown below and provide value for each metric.\n",
    "Provide justification for each metric value.\n",
    "\n",
    "Metrics:\n",
    "- greeting: Did the agent greet the customer?\n",
    "- hold: Did the agent put the customer on hold?\n",
    "- ticket: Did the agent create a ticket?\n",
    "- listening: How well did the agent listen to the customer?\n",
    "- understanding: How well did the agent understand the customer?\n",
    "- tone: How was the agent's tone?\n",
    "- proactivity: How proactive was the agent?\n",
    "- clarity: How clear was the agent's communication?\n",
    "- resolved: Was the issue resolved?\n",
    "- customer_sentiment: Customer sentiment score from 1 to 10\n",
    "- escalation: Was the call escalated?\n",
    "- agent_feedback: Feedback for the agent for handling similar calls in the future\n",
    "- escalation_reason: If the call was escalated, provide the reason for escalation\n",
    "\n",
    "Here is the call transcript:\n",
    "###\n",
    "{transcript}\n",
    "###\n",
    "\"\"\"\n",
    "\n",
    "llm = ChatNVIDIA(model=\"meta/llama-3.3-70b-instruct\", api_key=os.environ[\"NVIDIA_LLAMA_NIM_API_KEY\"])\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "chain = prompt | llm.with_structured_output(Evaluation) \n",
    "transcript_text = \"\\n\".join([f\"{u.time} - {u.speaker}: {u.spoken_words}\" for u in transcript.utterances])\n",
    "\n",
    "params = {\"transcript\": transcript_text}\n",
    "evaluation: Evaluation = chain.invoke(params)\n",
    "print(evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Prepare the result and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Result(BaseModel):\n",
    "    transcript: Transcript\n",
    "    entities: Entities\n",
    "    evaluation: Evaluation\n",
    "\n",
    "result = Result(transcript=transcript, entities=entities, evaluation=evaluation)\n",
    "\n",
    "# check if the results directory exists\n",
    "if not os.path.exists(\"results\"):\n",
    "    os.makedirs(\"results\")\n",
    "    \n",
    "with open(f\"results/{AUDIO_FILE}.json\", \"w\") as f:\n",
    "    f.write(result.model_dump_json(indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Django Shell-Plus",
   "language": "python",
   "name": "django_extensions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
