{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contact Center Insights\n",
    "\n",
    "This notebook demonstrates extracting valuable insights for contact centers using NVIDIA Riva and NVIDIA NIM microservices. \n",
    "\n",
    "Utilizing NVIDIA's Parakeet CTC 1.1b ASR model, it accurately transcribes audio interactions between two speakers. Subsequently, NVIDIA NIM Llama 3.3 70B processes the transcripts to extract key entities and evaluate agent performance, providing actionable insights to enhance contact center operations.\n",
    "\n",
    "Here is an architecture diagram of the workflow:\n",
    "\n",
    "![Contact Center Insights Architecture Diagram](./Architecture_Diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contact Center Insights generation involves two steps:\n",
    "\n",
    "1. **Transcription with speaker Diarization**\n",
    "   - **NVIDIA Riva Integration:** Transcribes incoming audio calls between two speakers using NVIDIA Riva's Parakeet CTC 1.1b ASR model and creates a structured transcript.\n",
    "\n",
    "2. **Insight Generation**\n",
    "   - **Entity Extraction:** Extracts key entities like customer and agent names, topic and subtopic of the conversation.\n",
    "    - **Agent Performance Evaluation:** Evaluates agent performance based several key metrics.\n",
    "    - **Combine Insights:** Combines all extracted insights into a structured JSON."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content Overview\n",
    "1. [Install dependencies](#Install-dependencies)\n",
    "2. [Set required environment variables](#Set-required-environment-variables)\n",
    "3. [Transcribe Audio](#Transcribe-Audio)\n",
    "4. [Generate Insights](#Generate-Insights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Set required environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from io import BytesIO\n",
    "from pydub import AudioSegment\n",
    "import riva.client\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# validate we have the required variables\n",
    "REQUIRED_VARIABLES = [\n",
    "    \"NVIDIA_PARAKEET_NIM_API_KEY\",\n",
    "    \"NVIDIA_LLAMA_NIM_API_KEY\",\n",
    "]\n",
    "\n",
    "for var in REQUIRED_VARIABLES:\n",
    "    if var not in os.environ:\n",
    "        os.environ[var] = getpass.getpass(f\"Please set the {var} environment variable.\")\n",
    "\n",
    "# optional variables\n",
    "os.environ[\"RIVA_SPEECH_API_SERVER\"] = os.getenv(\"RIVA_SPEECH_API_SERVER\", \"grpc.nvcf.nvidia.com\")\n",
    "\n",
    "# Look for audio files in the current directory with .wav format\n",
    "audio_files = [f for f in os.listdir(\"audio\") if f.endswith(\".wav\")]\n",
    "if not audio_files:\n",
    "    raise Exception(\"No .wav files found in the current directory.\")\n",
    "\n",
    "AUDIO_FILE = audio_files[0]\n",
    "print(f\"Using audio file: {AUDIO_FILE}\")\n",
    "\n",
    "# validate the audio file, it must have two channels\n",
    "audio = AudioSegment.from_file(f\"audio/{AUDIO_FILE}\")\n",
    "if audio.channels != 2:\n",
    "    raise Exception(\"Audio file must have exactly two channels.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Transcribe Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_audio_channels(filename: str) -> tuple[BytesIO, BytesIO]:\n",
    "    \"\"\"Split the audio file into two channels.\"\"\"\n",
    "    audio = AudioSegment.from_file(f\"audio/{filename}\", format=\"wav\")\n",
    "\n",
    "    left_channel = audio.split_to_mono()[0]\n",
    "    right_channel = audio.split_to_mono()[1]\n",
    "\n",
    "    left_channel_bytes, right_channel_bytes = BytesIO(), BytesIO()\n",
    "    left_channel.export(left_channel_bytes, format=\"wav\")\n",
    "    right_channel.export(right_channel_bytes, format=\"wav\")\n",
    "\n",
    "    return left_channel_bytes, right_channel_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_with_riva(audio_bytes):\n",
    "    \"\"\"Transcribe the audio file using Riva Speech API.\"\"\"\n",
    "\n",
    "    # Authenticate with Riva Speech API\n",
    "    auth = riva.client.Auth(\n",
    "        uri=os.environ[\"RIVA_SPEECH_API_SERVER\"],\n",
    "        use_ssl=True,\n",
    "        metadata_args=[\n",
    "            ['authorization', 'Bearer {}'.format(os.environ[\"NVIDIA_PARAKEET_NIM_API_KEY\"])],\n",
    "            ['function-id', '1598d209-5e27-4d3c-8079-4751568b1081']\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Configure the transcription\n",
    "    config = riva.client.RecognitionConfig(\n",
    "        language_code=\"en-US\",\n",
    "        enable_word_time_offsets=True,      # Enables word timestamps\n",
    "        max_alternatives=1,                 # Set to 1 for single-best result\n",
    "        enable_automatic_punctuation=True,\n",
    "        audio_channel_count = 1,\n",
    "    )\n",
    "\n",
    "    riva_asr = riva.client.ASRService(auth)\n",
    "    response = riva_asr.offline_recognize(audio_bytes, config)\n",
    "    \n",
    "    return response.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_and_format_results(left_results, right_results):\n",
    "    \"\"\"Combine the results from the two channels and format them.\"\"\"\n",
    "    \n",
    "    def extract_transcript(results, speaker_label):\n",
    "        \"\"\"Extract the transcript and start time from the first word of each alternative.\"\"\"\n",
    "\n",
    "        transcript_results = []\n",
    "        for result in results:\n",
    "            for alternative in result.alternatives:\n",
    "                transcript_results.append({\n",
    "                    'transcript': alternative.transcript,\n",
    "                    'start_time': alternative.words[0].start_time, # Start time of the first word\n",
    "                    'speaker': speaker_label\n",
    "                })\n",
    "\n",
    "        return transcript_results\n",
    "\n",
    "    left_results = extract_transcript(left_results, 'Speaker 1')\n",
    "    right_results = extract_transcript(right_results, 'Speaker 2')\n",
    "\n",
    "    combined_results = left_results + right_results\n",
    "    # sort all utterances by start_time\n",
    "    combined_results.sort(key=lambda x: x['start_time'])\n",
    "\n",
    "    # convert start_time to hh:mm:ss format\n",
    "    for result in combined_results:\n",
    "        seconds = result['start_time'] // 1000\n",
    "        hours, remainder = divmod(seconds, 3600)\n",
    "        minutes, seconds = divmod(remainder, 60)\n",
    "        result['start_time'] = f\"{hours:02}:{minutes:02}:{seconds:02}\"\n",
    "\n",
    "    # format the results\n",
    "    utterances = []\n",
    "    for result in combined_results:\n",
    "        utterances.append(f\"{result['start_time']} - {result['speaker']}: {result['transcript']}\")\n",
    "\n",
    "    return utterances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the audio file into two channels\n",
    "left_channel, right_channel = split_audio_channels(AUDIO_FILE)\n",
    "\n",
    "# transcribe the both channels individually\n",
    "left_results = transcribe_with_riva(left_channel.getvalue())\n",
    "right_results = transcribe_with_riva(right_channel.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine and format the results\n",
    "CALL_TRANSCRIPT = combine_and_format_results(left_results, right_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Generate Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:  Set up nvidia langchain with access to the llama model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:  Prompt llama 3.3 70B to generate entities - Agent name, Customer name, Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:  Prompt llama 3.3 70B to generate agent performance metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Django Shell-Plus",
   "language": "python",
   "name": "django_extensions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
